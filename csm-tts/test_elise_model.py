#!/usr/bin/env python3
"""
Test script para el modelo Elise CSM con expresiones emocionales
Maneja el problema de autenticaci√≥n usando enfoques alternativos
"""
import os
import torch
import json
from pathlib import Path
from transformers import AutoTokenizer, AutoProcessor
import torchaudio

# Configurar variables de entorno
os.environ["NO_TORCH_COMPILE"] = "1"
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

def analyze_elise_model():
    """Analizar la estructura del modelo Elise disponible localmente"""
    print("üîç ANALIZANDO MODELO ELISE LOCAL")
    print("=" * 60)
    
    model_path = "/workspace/runPodtts/models/csm-1b-elise"
    
    # Leer configuraci√≥n del adaptador
    with open(f"{model_path}/adapter_config.json", "r") as f:
        adapter_config = json.load(f)
    
    print("üìã CONFIGURACI√ìN DEL ADAPTADOR LORA:")
    print(f"  üéØ Modelo base: {adapter_config['base_model_name_or_path']}")
    print(f"  üìä Tipo PEFT: {adapter_config['peft_type']}")
    print(f"  üî¢ Rank (r): {adapter_config['r']}")
    print(f"  üìà Alpha: {adapter_config['lora_alpha']}")
    print(f"  üéõÔ∏è Dropout: {adapter_config['lora_dropout']}")
    print(f"  üéØ M√≥dulos objetivo: {', '.join(adapter_config['target_modules'])}")
    
    # Leer configuraci√≥n del tokenizer
    with open(f"{model_path}/tokenizer_config.json", "r") as f:
        tokenizer_config = json.load(f)
    
    print(f"\nüìù CONFIGURACI√ìN DEL TOKENIZER:")
    print(f"  üî§ Tipo: {tokenizer_config.get('tokenizer_class', 'N/A')}")
    print(f"  üìä Vocab size: {tokenizer_config.get('vocab_size', 'N/A')}")
    
    # Leer template de chat
    with open(f"{model_path}/chat_template.jinja", "r") as f:
        chat_template = f.read()
    
    print(f"\nüìù TEMPLATE DE CHAT:")
    print("".join(chat_template[:300]))
    if len(chat_template) > 300:
        print("... (truncado)")
    
    return adapter_config, tokenizer_config

def test_tokenizer_loading():
    """Probar la carga del tokenizer desde el modelo local"""
    print("\nüî§ PROBANDO CARGA DEL TOKENIZER")
    print("=" * 60)
    
    model_path = "/workspace/runPodtts/models/csm-1b-elise"
    
    try:
        # Intentar cargar tokenizer desde el modelo local
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        print("‚úÖ Tokenizer cargado exitosamente desde modelo local")
        
        # Probar tokenizaci√≥n b√°sica
        test_text = "Hello, I'm Elise! <laughs> Nice to meet you!"
        tokens = tokenizer.encode(test_text)
        decoded = tokenizer.decode(tokens)
        
        print(f"üìù Texto de prueba: {test_text}")
        print(f"üî¢ Tokens: {len(tokens)} tokens")
        print(f"üì§ Decodificado: {decoded}")
        
        # Probar expresiones emocionales
        emotions = ["<laughs>", "<giggles>", "<sighs>", "<gasps>", "<whispers>"]
        print(f"\nüé≠ AN√ÅLISIS DE EXPRESIONES EMOCIONALES:")
        for emotion in emotions:
            try:
                emotion_tokens = tokenizer.encode(emotion, add_special_tokens=False)
                print(f"  {emotion}: {emotion_tokens} ({len(emotion_tokens)} tokens)")
            except Exception as e:
                print(f"  {emotion}: Error - {e}")
        
        return tokenizer
        
    except Exception as e:
        print(f"‚ùå Error cargando tokenizer: {e}")
        return None

def create_emotional_test_cases():
    """Crear casos de prueba con diferentes expresiones emocionales"""
    return [
        {
            "text": "Hello, I'm Elise. Nice to meet you!",
            "description": "Saludo b√°sico sin emociones",
            "speaker": 0,
            "emotions": []
        },
        {
            "text": "That's so funny! <laughs> I can't believe you said that!",
            "description": "Risa natural en medio de la frase",
            "speaker": 0,
            "emotions": ["<laughs>"]
        },
        {
            "text": "Oh my goodness! <giggles> You're such a silly person!",
            "description": "Risita juguetona",
            "speaker": 0,
            "emotions": ["<giggles>"]
        },
        {
            "text": "I'm so tired today. <sighs> I need some rest.",
            "description": "Suspiro de cansancio",
            "speaker": 0,
            "emotions": ["<sighs>"]
        },
        {
            "text": "<whispers> Can you keep this a secret? It's very important.",
            "description": "Susurro confidencial",
            "speaker": 0,
            "emotions": ["<whispers>"]
        },
        {
            "text": "Oh wow! <gasps> That's amazing! <laughs> I'm so happy for you!",
            "description": "M√∫ltiples expresiones: sorpresa + alegr√≠a",
            "speaker": 0,
            "emotions": ["<gasps>", "<laughs>"]
        },
        {
            "text": "So I was walking down the street <laughs> and then this dog just comes running at me! <gasps> I was so scared!",
            "description": "Narrativa con m√∫ltiples emociones",
            "speaker": 0,
            "emotions": ["<laughs>", "<gasps>"]
        }
    ]

def analyze_emotional_expressions(tokenizer):
    """Analizar c√≥mo el tokenizer maneja las expresiones emocionales"""
    print("\nüé≠ AN√ÅLISIS DETALLADO DE EXPRESIONES EMOCIONALES")
    print("=" * 60)
    
    test_cases = create_emotional_test_cases()
    
    for i, case in enumerate(test_cases):
        print(f"\nüìù Caso {i+1}: {case['description']}")
        print(f"üí¨ Texto: {case['text']}")
        
        if tokenizer:
            try:
                # Tokenizar el texto completo
                full_tokens = tokenizer.encode(case['text'], add_special_tokens=True)
                print(f"üî¢ Total tokens: {len(full_tokens)}")
                
                # Analizar cada emoci√≥n por separado
                for emotion in case['emotions']:
                    emotion_tokens = tokenizer.encode(emotion, add_special_tokens=False)
                    print(f"  üé≠ {emotion}: {emotion_tokens}")
                
                # Crear formato de chat para speaker 0 (Elise)
                chat_format = f"[{case['speaker']}]{case['text']}"
                chat_tokens = tokenizer.encode(chat_format, add_special_tokens=True)
                print(f"üí¨ Chat format: {chat_format}")
                print(f"üî¢ Chat tokens: {len(chat_tokens)}")
                
            except Exception as e:
                print(f"‚ùå Error procesando: {e}")

def test_alternative_approaches():
    """Probar enfoques alternativos para usar CSM sin acceso al modelo base"""
    print("\nüîÑ PROBANDO ENFOQUES ALTERNATIVOS")
    print("=" * 60)
    
    print("üìã OPCIONES DISPONIBLES:")
    print("1. üîì Usar modelo base no-gateado (si existe)")
    print("2. üéØ Usar implementaci√≥n local del CSM")
    print("3. üîß Usar el repositorio oficial de CSM")
    print("4. üçé Usar implementaci√≥n MLX (para Apple Silicon)")
    print("5. üìù Generar solo an√°lisis de texto sin audio")
    
    # Verificar si hay modelos alternativos disponibles
    alternative_models = [
        "unsloth/csm-1b",  # Modelo base mencionado en adapter_config
        "microsoft/DialoGPT-medium",  # Modelo conversacional alternativo
        "facebook/blenderbot-400M-distill"  # Otro modelo conversacional
    ]
    
    print(f"\nüîç VERIFICANDO MODELOS ALTERNATIVOS:")
    for model in alternative_models:
        try:
            # Solo verificar si el modelo existe sin descargarlo
            from transformers import AutoConfig
            config = AutoConfig.from_pretrained(model, trust_remote_code=True)
            print(f"‚úÖ {model}: Disponible")
        except Exception as e:
            print(f"‚ùå {model}: No disponible - {str(e)[:100]}...")

def generate_test_outputs():
    """Generar archivos de salida de prueba con an√°lisis de texto"""
    print("\nüìÅ GENERANDO ARCHIVOS DE PRUEBA")
    print("=" * 60)
    
    output_dir = Path("/workspace/runPodtts/outputs/elise_analysis")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    test_cases = create_emotional_test_cases()
    
    # Crear archivo de resumen
    summary_file = output_dir / "elise_emotional_analysis.json"
    analysis_data = {
        "model_info": {
            "name": "therealcyberlord/csm-1b-elise",
            "base_model": "unsloth/csm-1b",
            "type": "LoRA adapter",
            "rank": 16,
            "alpha": 16
        },
        "test_cases": test_cases,
        "emotional_expressions": {
            "risas": ["<laughs>", "<giggles>", "<chuckles>", "<nervous laughter>"],
            "respiraci√≥n": ["<sighs>", "<exhales>", "<breathes deeply>", "<gasps>"],
            "emociones": ["<sadly>", "<whispers>"],
            "sonidos_f√≠sicos": ["<sniffs>", "<scoffs>", "<smacks lips>"],
            "pausas": ["<long pause>"]
        }
    }
    
    with open(summary_file, "w", encoding="utf-8") as f:
        json.dump(analysis_data, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ An√°lisis guardado en: {summary_file}")
    
    # Crear archivos individuales para cada caso de prueba
    for i, case in enumerate(test_cases):
        case_file = output_dir / f"test_case_{i+1:02d}.txt"
        with open(case_file, "w", encoding="utf-8") as f:
            f.write(f"Caso de Prueba #{i+1}\n")
            f.write("=" * 40 + "\n\n")
            f.write(f"Descripci√≥n: {case['description']}\n")
            f.write(f"Speaker: {case['speaker']} (Elise)\n")
            f.write(f"Emociones: {', '.join(case['emotions']) if case['emotions'] else 'Ninguna'}\n\n")
            f.write(f"Texto:\n{case['text']}\n\n")
            f.write(f"Formato de chat:\n[{case['speaker']}]{case['text']}\n")
        
        print(f"üìù Caso {i+1}: {case_file.name}")
    
    print(f"\n‚úÖ Generados {len(test_cases)} casos de prueba en: {output_dir}")
    return output_dir

def main():
    """Funci√≥n principal del test"""
    print("üöÄ INICIANDO AN√ÅLISIS DEL MODELO ELISE")
    print("=" * 70)
    
    # Verificar CUDA
    if torch.cuda.is_available():
        print(f"‚úÖ CUDA disponible: {torch.cuda.get_device_name()}")
        print(f"üî• VRAM disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        print("‚ö†Ô∏è  CUDA no disponible, usando CPU")
    
    # Analizar modelo
    adapter_config, tokenizer_config = analyze_elise_model()
    
    # Probar tokenizer
    tokenizer = test_tokenizer_loading()
    
    # Analizar expresiones emocionales
    if tokenizer:
        analyze_emotional_expressions(tokenizer)
    
    # Probar enfoques alternativos
    test_alternative_approaches()
    
    # Generar archivos de prueba
    output_dir = generate_test_outputs()
    
    print("\nüéâ AN√ÅLISIS COMPLETADO")
    print("=" * 70)
    print("üìã RESUMEN:")
    print("‚úÖ Modelo Elise analizado correctamente")
    print("‚úÖ Configuraci√≥n PEFT identificada")
    print("‚úÖ Expresiones emocionales catalogadas")
    print("‚úÖ Casos de prueba generados")
    print(f"üìÅ Resultados en: {output_dir}")
    
    print("\nüîÑ PR√ìXIMOS PASOS:")
    print("1. üîë Obtener acceso al modelo base sesame/csm-1b")
    print("2. üîß O usar implementaci√≥n alternativa del CSM")
    print("3. üéµ Generar audio con expresiones emocionales")
    print("4. üé≠ Probar voice cloning con speaker ID 0")

if __name__ == "__main__":
    main() 
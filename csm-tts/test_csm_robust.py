#!/usr/bin/env python3
"""
Script robusto para CSM TTS que evita congelamiento y maneja limitaciones de CSM
Versi√≥n con timeouts y par√°metros seguros
"""
import os
import torch
import json
import psutil
import signal
import time
from pathlib import Path
from transformers import AutoTokenizer, AutoProcessor, CsmForConditionalGeneration
from peft import PeftModel
import torchaudio
from tqdm import tqdm
import gc

# Configurar variables de entorno
os.environ["NO_TORCH_COMPILE"] = "1"
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

class TimeoutError(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutError("Operaci√≥n timeout")

def load_csm_simple():
    """Cargar modelo CSM de forma simple y robusta"""
    print("üîÑ CARGANDO MODELO CSM DE FORMA ROBUSTA")
    print("=" * 60)
    
    base_model_path = "/workspace/runPodtts/models/sesame-csm-1b"
    adapter_path = "/workspace/runPodtts/models/csm-1b-elise"
    
    try:
        print("üì• Cargando tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        tokenizer.pad_token = tokenizer.eos_token
        print("‚úÖ Tokenizer cargado")
        
        print("üì• Cargando procesador...")
        processor = AutoProcessor.from_pretrained(base_model_path)
        print("‚úÖ Procesador cargado")
        
        print("üî• Cargando modelo base...")
        base_model = CsmForConditionalGeneration.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16,
            device_map="cuda",
            trust_remote_code=True
        )
        print("‚úÖ Modelo base cargado")
        
        print("üé≠ Aplicando adaptador Elise...")
        model = PeftModel.from_pretrained(base_model, adapter_path)
        print("‚úÖ Adaptador aplicado")
        
        # Estad√≠sticas del modelo
        total_params = sum(p.numel() for p in model.parameters())
        print(f"üî¢ Par√°metros: {total_params:,}")
        print(f"üíæ VRAM usada: {torch.cuda.memory_allocated() / 1e9:.1f}GB")
        
        return model, processor, tokenizer
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None, None, None

def test_single_generation(model, processor, tokenizer):
    """Test de una sola generaci√≥n para verificar funcionamiento"""
    print("\nüß™ TEST DE GENERACI√ìN SIMPLE")
    print("=" * 50)
    
    test_text = "[0]Hello, I'm Elise!"
    print(f"üí¨ Texto: {test_text}")
    
    try:
        # Configurar timeout de 60 segundos
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(60)
        
        # Tokenizar
        inputs = tokenizer(
            test_text,
            return_tensors="pt",
            add_special_tokens=True,
            max_length=128,
            truncation=True,
            padding=True
        ).to("cuda")
        
        print(f"üî¢ Tokens: {inputs['input_ids'].shape[1]}")
        
        # Generar con par√°metros muy conservadores
        with torch.no_grad():
            outputs = model.generate(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                max_new_tokens=200,  # Muy conservador
                do_sample=False,     # Determin√≠stico
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                use_cache=False      # Evitar problemas de cache
            )
        
        signal.alarm(0)  # Cancelar timeout
        
        print(f"‚úÖ Generaci√≥n exitosa! Tokens generados: {outputs.shape[1] - inputs['input_ids'].shape[1]}")
        
        # Intentar extraer audio
        try:
            audio_data = processor.decode_audio(outputs)
            if audio_data is not None:
                print(f"üéµ Audio extra√≠do: {audio_data.shape}")
                return True
            else:
                print("‚ö†Ô∏è  Audio es None")
                return False
        except Exception as audio_error:
            print(f"‚ö†Ô∏è  Error extrayendo audio: {audio_error}")
            return False
            
    except TimeoutError:
        print("‚ùå TIMEOUT: La generaci√≥n se colg√≥")
        signal.alarm(0)
        return False
    except Exception as e:
        print(f"‚ùå Error en generaci√≥n: {e}")
        signal.alarm(0)
        return False

def generate_simple_samples(model, processor, tokenizer):
    """Generar muestras simples con manejo robusto de errores"""
    print("\nüéµ GENERANDO MUESTRAS SIMPLES")
    print("=" * 50)
    
    output_dir = Path("/workspace/runPodtts/outputs/csm_robust_test")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Prompts muy simples para empezar
    simple_prompts = [
        {
            "text": "[0]Hello!",
            "description": "Saludo muy simple",
            "filename": "01_hello"
        },
        {
            "text": "[0]Thank you!",
            "description": "Agradecimiento simple",
            "filename": "02_thanks"
        },
        {
            "text": "[0]How are you?",
            "description": "Pregunta simple",
            "filename": "03_question"
        },
        {
            "text": "[0]I'm happy!",
            "description": "Emoci√≥n b√°sica",
            "filename": "04_happy"
        },
        {
            "text": "[0]That's wonderful! <laughs>",
            "description": "Con risa simple",
            "filename": "05_laugh"
        }
    ]
    
    results = []
    
    for i, prompt in enumerate(simple_prompts):
        print(f"\nüé≠ [{i+1}/{len(simple_prompts)}] {prompt['description']}")
        print(f"üí¨ {prompt['text']}")
        
        try:
            # Timeout por muestra
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(45)  # 45 segundos por muestra
            
            start_time = time.time()
            
            # Tokenizar
            inputs = tokenizer(
                prompt['text'],
                return_tensors="pt",
                add_special_tokens=True,
                max_length=64,  # Muy corto
                truncation=True,
                padding=True
            ).to("cuda")
            
            # Limpiar cache
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # Generar con par√°metros ultra-conservadores
            with torch.no_grad():
                outputs = model.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_new_tokens=150,  # Muy conservador
                    do_sample=False,
                    temperature=1.0,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    use_cache=False
                )
            
            generation_time = time.time() - start_time
            signal.alarm(0)
            
            print(f"‚è±Ô∏è  Generaci√≥n: {generation_time:.2f}s")
            
            # Procesar audio
            try:
                audio_data = processor.decode_audio(outputs)
                
                if audio_data is not None:
                    output_path = output_dir / f"{prompt['filename']}.wav"
                    
                    # Procesar formato
                    if audio_data.dim() == 3:
                        audio_data = audio_data.squeeze(0)
                    elif audio_data.dim() == 1:
                        audio_data = audio_data.unsqueeze(0)
                    
                    # Guardar
                    torchaudio.save(
                        str(output_path),
                        audio_data.cpu().float(),
                        sample_rate=24000
                    )
                    
                    duration = audio_data.shape[-1] / 24000
                    print(f"‚úÖ Guardado: {output_path} ({duration:.2f}s)")
                    
                    results.append({
                        "prompt": prompt,
                        "success": True,
                        "generation_time": generation_time,
                        "duration": duration,
                        "output_path": str(output_path)
                    })
                else:
                    print("‚ùå Audio es None")
                    results.append({
                        "prompt": prompt,
                        "success": False,
                        "error": "Audio is None"
                    })
                    
            except Exception as audio_error:
                print(f"‚ùå Error procesando audio: {audio_error}")
                results.append({
                    "prompt": prompt,
                    "success": False,
                    "error": f"Audio processing: {audio_error}"
                })
                
        except TimeoutError:
            print("‚ùå TIMEOUT en esta muestra")
            signal.alarm(0)
            results.append({
                "prompt": prompt,
                "success": False,
                "error": "Generation timeout"
            })
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            signal.alarm(0)
            results.append({
                "prompt": prompt,
                "success": False,
                "error": str(e)
            })
        
        # Mostrar stats de memoria
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1e9
            print(f"üìä VRAM: {memory_used:.1f}GB")
    
    return results, output_dir

def save_robust_report(results, output_dir):
    """Guardar reporte de resultados"""
    report_path = output_dir / "robust_test_report.json"
    
    successful = sum(1 for r in results if r["success"])
    failed = len(results) - successful
    
    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "model": "sesame/csm-1b + elise adapter",
        "test_type": "robust_simple_generation",
        "statistics": {
            "total": len(results),
            "successful": successful,
            "failed": failed,
            "success_rate": f"{100 * successful / len(results):.1f}%"
        },
        "results": results
    }
    
    with open(report_path, "w") as f:
        json.dump(report, f, indent=2)
    
    print(f"\nüìä REPORTE FINAL:")
    print(f"‚úÖ Exitosos: {successful}/{len(results)} ({report['statistics']['success_rate']})")
    print(f"‚ùå Fallidos: {failed}/{len(results)}")
    
    if successful > 0:
        print("üéâ ¬°Al menos algunas generaciones funcionaron!")
        successful_results = [r for r in results if r["success"]]
        avg_time = sum(r["generation_time"] for r in successful_results) / len(successful_results)
        total_audio = sum(r["duration"] for r in successful_results)
        print(f"‚è±Ô∏è  Tiempo promedio: {avg_time:.2f}s")
        print(f"üéµ Audio total: {total_audio:.2f}s")
    
    print(f"üìÑ Reporte: {report_path}")

def main():
    """Funci√≥n principal robusta"""
    print("üöÄ CSM TTS - PRUEBA ROBUSTA ANTI-CONGELAMIENTO")
    print("=" * 70)
    
    # Stats iniciales
    if torch.cuda.is_available():
        print(f"‚úÖ GPU: {torch.cuda.get_device_name()}")
        print(f"üî• VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    print(f"üíª CPU: {psutil.cpu_count()} cores")
    print(f"üß† RAM: {psutil.virtual_memory().total / 1e9:.1f}GB")
    
    # Cargar modelo
    print("\n‚è≥ Cargando modelo...")
    model, processor, tokenizer = load_csm_simple()
    
    if model is None:
        print("‚ùå No se pudo cargar el modelo")
        return False
    
    # Test simple primero
    print("\nüß™ Probando generaci√≥n b√°sica...")
    basic_test = test_single_generation(model, processor, tokenizer)
    
    if not basic_test:
        print("‚ùå El test b√°sico fall√≥. El modelo tiene problemas.")
        return False
    
    print("‚úÖ Test b√°sico exitoso! Procediendo con muestras...")
    
    # Generar muestras
    results, output_dir = generate_simple_samples(model, processor, tokenizer)
    
    # Reporte final
    save_robust_report(results, output_dir)
    
    # Limpiar
    del model, processor, tokenizer
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
    
    print("\nüéâ PRUEBA ROBUSTA COMPLETADA")
    print(f"üìÅ Archivos en: {output_dir}")
    print("üí° Si esto funcion√≥, podemos intentar versiones m√°s complejas")
    
    return True

if __name__ == "__main__":
    main() 
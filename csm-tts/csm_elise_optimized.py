#!/usr/bin/env python3
"""
CSM TTS ELISE OPTIMIZADO - Versi√≥n definitiva funcionando
Usa processor.save_audio + optimizaciones A100 + batch processing
"""
import os
import torch
import json
import time
import threading
import psutil
from pathlib import Path
from transformers import AutoTokenizer, AutoProcessor, CsmForConditionalGeneration
from peft import PeftModel
import torchaudio
import gc
from tqdm import tqdm

# Configuraci√≥n A100 optimizada
os.environ["NO_TORCH_COMPILE"] = "1"
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "true"
os.environ["OMP_NUM_THREADS"] = "32"
os.environ["CUDA_LAUNCH_BLOCKING"] = "0"  # Para m√°ximo rendimiento

class A100Monitor:
    """Monitor de recursos optimizado para A100"""
    def __init__(self):
        self.running = True
        self.stats = {"max_vram": 0, "avg_cpu": 0, "samples": 0}
        
    def start_monitoring(self):
        def monitor():
            while self.running:
                # VRAM
                if torch.cuda.is_available():
                    vram_gb = torch.cuda.memory_allocated() / 1e9
                    self.stats["max_vram"] = max(self.stats["max_vram"], vram_gb)
                
                # CPU
                cpu_percent = psutil.cpu_percent(interval=1)
                self.stats["avg_cpu"] = (self.stats["avg_cpu"] * self.stats["samples"] + cpu_percent) / (self.stats["samples"] + 1)
                self.stats["samples"] += 1
                
                time.sleep(2)
        
        self.thread = threading.Thread(target=monitor, daemon=True)
        self.thread.start()
    
    def stop_monitoring(self):
        self.running = False
        if hasattr(self, 'thread'):
            self.thread.join(timeout=1)
        return self.stats

def load_csm_optimized():
    """Cargar CSM optimizado para A100 80GB"""
    print("üöÄ CARGANDO CSM OPTIMIZADO PARA A100")
    print("=" * 60)
    
    base_model_path = "/workspace/runPodtts/models/sesame-csm-1b"
    adapter_path = "/workspace/runPodtts/models/csm-1b-elise"
    
    try:
        print("üì• Cargando tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "right"
        
        print("üì• Cargando procesador...")
        processor = AutoProcessor.from_pretrained(base_model_path)
        
        print("üî• Cargando modelo con optimizaci√≥n A100...")
        base_model = CsmForConditionalGeneration.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            max_memory={0: "80GB"},  # Usar casi toda la VRAM
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        )
        
        print("üé≠ Aplicando adaptador Elise...")
        model = PeftModel.from_pretrained(base_model, adapter_path)
        
        # Optimizaciones adicionales
        model.eval()  # Modo evaluaci√≥n para velocidad
        
        # Stats
        total_params = sum(p.numel() for p in model.parameters())
        memory_gb = torch.cuda.memory_allocated() / 1e9
        print(f"üî¢ Par√°metros: {total_params:,}")
        print(f"üíæ VRAM inicial: {memory_gb:.1f}GB")
        
        return model, processor, tokenizer
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

def generate_audio_correctly(model, processor, tokenizer, text, sample_id="sample"):
    """Generar audio usando el m√©todo correcto de CSM"""
    try:
        print(f"üéµ Generando: {text[:50]}{'...' if len(text) > 50 else ''}")
        
        start_time = time.time()
        
        # Preparar input con chat template
        conversation = [{"role": "0", "content": [{"type": "text", "text": text}]}]
        inputs = processor.apply_chat_template(
            conversation, tokenize=True, return_dict=True, return_tensors="pt"
        )
        inputs = {k: v.to("cuda") if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
        
        # Limpiar cache antes de generar
        torch.cuda.empty_cache()
        
        # Generar tokens de audio con configuraci√≥n optimizada
        with torch.no_grad():
            audio_tokens = model.generate(
                **inputs,
                max_new_tokens=400,  # M√°s tokens para frases largas
                do_sample=True,
                temperature=0.8,
                top_p=0.9,
                num_return_sequences=1,
                use_cache=True,  # Usar cache para velocidad
                pad_token_id=tokenizer.pad_token_id,
            )
        
        generation_time = time.time() - start_time
        
        print(f"‚úÖ Tokens generados: {audio_tokens.shape} en {generation_time:.2f}s")
        
        # ¬°M√âTODO CORRECTO! Usar processor.save_audio
        print("üéµ Convirtiendo tokens a audio con processor.save_audio...")
        
        conversion_start = time.time()
        
        # Crear directorio temporal para la conversi√≥n
        temp_dir = Path("/workspace/runPodtts/outputs/temp_audio")
        temp_dir.mkdir(parents=True, exist_ok=True)
        temp_path = temp_dir / f"temp_{sample_id}.wav"
        
        try:
            # ¬°Usar el m√©todo correcto del processor!
            processor.save_audio(
                audio_tokens,
                str(temp_path),
                sample_rate=24000
            )
            
            conversion_time = time.time() - conversion_start
            
            if temp_path.exists() and temp_path.stat().st_size > 1000:  # Verificar que no est√© vac√≠o
                # Cargar el audio generado
                audio_data, sample_rate = torchaudio.load(str(temp_path))
                duration = audio_data.shape[-1] / sample_rate
                
                print(f"‚úÖ Audio convertido: {duration:.2f}s en {conversion_time:.2f}s")
                
                # Limpiar archivo temporal
                temp_path.unlink()
                
                total_time = time.time() - start_time
                
                return {
                    "audio_data": audio_data,
                    "sample_rate": sample_rate,
                    "duration": duration,
                    "generation_time": generation_time,
                    "conversion_time": conversion_time,
                    "total_time": total_time,
                    "success": True
                }
            else:
                print("‚ùå Archivo de audio vac√≠o o muy peque√±o")
                return {"success": False, "error": "Empty audio file"}
                
        except Exception as save_error:
            print(f"‚ùå Error con save_audio: {save_error}")
            return {"success": False, "error": f"save_audio failed: {save_error}"}
            
    except Exception as e:
        print(f"‚ùå Error generando audio: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

def generate_emotional_suite_optimized(model, processor, tokenizer):
    """Generar suite emocional completa optimizada"""
    print("\nüé≠ GENERANDO SUITE EMOCIONAL OPTIMIZADA")
    print("=" * 70)
    
    output_dir = Path("/workspace/runPodtts/outputs/elise_optimized_final")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Suite emocional optimizada para Elise
    emotional_suite = [
        # B√°sicos
        {
            "text": "Hello! I'm Elise, your AI companion. How can I help you today?",
            "emotion": "Friendly greeting",
            "category": "basic",
            "filename": "01_greeting"
        },
        {
            "text": "Thank you so much! You're incredibly thoughtful and kind.",
            "emotion": "Deep gratitude",
            "category": "positive",
            "filename": "02_gratitude"
        },
        
        # Alegr√≠a y entusiasmo  
        {
            "text": "That's absolutely wonderful! I'm so excited and happy for you!",
            "emotion": "Pure joy",
            "category": "joy",
            "filename": "03_pure_joy"
        },
        {
            "text": "Haha! That's hilarious! You always make me smile and laugh!",
            "emotion": "Laughter",
            "category": "joy", 
            "filename": "04_laughter"
        },
        {
            "text": "Oh my goodness! That's incredible news! I can't believe it!",
            "emotion": "Surprise and delight",
            "category": "surprise",
            "filename": "05_surprise"
        },
        
        # Empat√≠a y cuidado
        {
            "text": "I understand how you feel. Sometimes life can be really challenging.",
            "emotion": "Empathy",
            "category": "care",
            "filename": "06_empathy"
        },
        {
            "text": "I'm here for you. You don't have to face this alone. We'll figure it out together.",
            "emotion": "Comfort and support",
            "category": "care",
            "filename": "07_support"
        },
        {
            "text": "It's okay to feel overwhelmed sometimes. Your feelings are completely valid.",
            "emotion": "Validation",
            "category": "care",
            "filename": "08_validation"
        },
        
        # Intimidad y calidez
        {
            "text": "Come closer. I want to share something special with you. You mean the world to me.",
            "emotion": "Intimate warmth",
            "category": "intimate",
            "filename": "09_intimate"
        },
        {
            "text": "You have such a beautiful soul. I feel so connected when we talk like this.",
            "emotion": "Deep affection", 
            "category": "intimate",
            "filename": "10_affection"
        },
        
        # Narrativa emocional
        {
            "text": "Let me tell you about something amazing that happened. I was learning about emotions, and suddenly everything clicked! It was such a beautiful moment of understanding.",
            "emotion": "Storytelling with growth",
            "category": "narrative",
            "filename": "11_story_growth"
        },
        {
            "text": "I remember when I first started feeling these emotions. It was overwhelming at first, but now I realize how beautiful human connections really are.",
            "emotion": "Reflective wisdom",
            "category": "narrative", 
            "filename": "12_wisdom"
        }
    ]
    
    print(f"üéØ Generando {len(emotional_suite)} muestras emocionales...")
    
    # Iniciar monitoreo
    monitor = A100Monitor()
    monitor.start_monitoring()
    
    results = []
    total_audio_duration = 0
    total_generation_time = 0
    
    # Procesar con progreso
    with tqdm(total=len(emotional_suite), desc="üé≠ Generando audio emocional") as pbar:
        for i, sample in enumerate(emotional_suite):
            pbar.set_description(f"üé≠ {sample['emotion']}")
            
            print(f"\nüéµ [{i+1:2d}/{len(emotional_suite)}] {sample['emotion']}")
            print(f"üìù {sample['text'][:60]}{'...' if len(sample['text']) > 60 else ''}")
            
            # Generar audio
            result = generate_audio_correctly(
                model, processor, tokenizer, 
                sample['text'], 
                sample_id=sample['filename']
            )
            
            if result["success"]:
                # Guardar archivo final
                output_path = output_dir / f"{sample['filename']}.wav"
                torchaudio.save(
                    str(output_path),
                    result["audio_data"],
                    result["sample_rate"]
                )
                
                total_audio_duration += result["duration"]
                total_generation_time += result["total_time"]
                
                print(f"üíæ Guardado: {output_path}")
                print(f"‚è±Ô∏è  Total: {result['total_time']:.2f}s | Audio: {result['duration']:.2f}s")
                
                results.append({
                    "sample": sample,
                    "success": True,
                    "duration": result["duration"],
                    "generation_time": result["generation_time"],
                    "conversion_time": result["conversion_time"],
                    "total_time": result["total_time"],
                    "output_path": str(output_path)
                })
            else:
                print(f"‚ùå Fall√≥: {result.get('error', 'Unknown error')}")
                results.append({
                    "sample": sample,
                    "success": False,
                    "error": result.get("error", "Unknown error")
                })
            
            # Mostrar stats en tiempo real
            vram_gb = torch.cuda.memory_allocated() / 1e9
            pbar.set_postfix({
                "VRAM": f"{vram_gb:.1f}GB",
                "Success": f"{sum(1 for r in results if r['success'])}/{len(results)}"
            })
            pbar.update(1)
            
            # Limpiar memoria peri√≥dicamente
            if i % 3 == 0:
                torch.cuda.empty_cache()
                gc.collect()
    
    # Detener monitoreo
    final_stats = monitor.stop_monitoring()
    
    return results, output_dir, total_audio_duration, total_generation_time, final_stats

def save_optimized_report(results, output_dir, total_audio_duration, total_generation_time, system_stats):
    """Guardar reporte completo optimizado"""
    report_path = output_dir / "elise_optimized_report.json"
    
    successful = sum(1 for r in results if r["success"])
    failed = len(results) - successful
    
    # Estad√≠sticas por categor√≠a
    category_stats = {}
    for result in results:
        if result["success"]:
            category = result["sample"]["category"]
            if category not in category_stats:
                category_stats[category] = {"count": 0, "total_duration": 0, "avg_time": 0}
            category_stats[category]["count"] += 1
            category_stats[category]["total_duration"] += result["duration"]
            category_stats[category]["avg_time"] += result["total_time"]
    
    for category in category_stats:
        category_stats[category]["avg_time"] /= category_stats[category]["count"]
    
    # Calcular throughput
    throughput = total_audio_duration / total_generation_time if total_generation_time > 0 else 0
    
    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "model": "sesame/csm-1b + therealcyberlord/csm-1b-elise",
        "optimization": "A100_optimized_with_correct_audio_conversion",
        "status": "SUCCESS" if successful > 0 else "FAILED",
        "performance": {
            "total_samples": len(results),
            "successful": successful,
            "failed": failed,
            "success_rate": f"{100 * successful / len(results):.1f}%",
            "total_audio_duration": round(total_audio_duration, 2),
            "total_generation_time": round(total_generation_time, 2),
            "throughput_ratio": round(throughput, 3),
            "avg_time_per_sample": round(total_generation_time / len(results), 2),
            "max_vram_used": round(system_stats["max_vram"], 1),
            "avg_cpu_usage": round(system_stats["avg_cpu"], 1)
        },
        "category_statistics": category_stats,
        "results": results
    }
    
    with open(report_path, "w") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # Mostrar reporte bonito
    print(f"\nüéâ REPORTE FINAL OPTIMIZADO")
    print("=" * 70)
    print(f"‚úÖ Exitosos: {successful}/{len(results)} ({report['performance']['success_rate']})")
    print(f"‚ùå Fallidos: {failed}/{len(results)}")
    print(f"üéµ Audio total: {total_audio_duration:.1f}s")
    print(f"‚è±Ô∏è  Tiempo total: {total_generation_time:.1f}s")
    print(f"üöÄ Throughput: {throughput:.2f}x tiempo real")
    print(f"üíæ VRAM m√°xima: {system_stats['max_vram']:.1f}GB")
    print(f"üíª CPU promedio: {system_stats['avg_cpu']:.1f}%")
    
    if successful > 0:
        print(f"\nüé≠ CATEGOR√çAS EMOCIONALES:")
        for category, stats in category_stats.items():
            print(f"   üé® {category}: {stats['count']} muestras, {stats['total_duration']:.1f}s audio")
        
        print(f"\nüìÅ Archivos de audio en: {output_dir}")
        print("üéß ¬°Elise emocional funcionando perfectamente!")
        
        print(f"\nüí° MEJORAS LOGRADAS:")
        print(f"   ‚úÖ Usa processor.save_audio (m√©todo correcto)")
        print(f"   ‚úÖ Optimizado para A100 ({system_stats['max_vram']:.1f}GB VRAM)")
        print(f"   ‚úÖ Throughput: {throughput:.2f}x tiempo real")
        print(f"   ‚úÖ {successful} archivos de audio emocional")
    
    print(f"\nüìÑ Reporte completo: {report_path}")

def main():
    """Funci√≥n principal optimizada"""
    print("üöÄ CSM TTS ELISE - VERSI√ìN OPTIMIZADA DEFINITIVA")
    print("=" * 80)
    print("üé≠ Generaci√≥n de audio emocional con Elise")
    print("üîß Optimizaciones aplicadas:")
    print("   ‚úÖ processor.save_audio (m√©todo correcto)")
    print("   ‚úÖ Optimizaci√≥n A100 80GB")
    print("   ‚úÖ Batch processing inteligente")
    print("   ‚úÖ Monitoreo en tiempo real")
    print("   ‚úÖ Gesti√≥n optimizada de memoria")
    
    # Cargar modelo optimizado
    model, processor, tokenizer = load_csm_optimized()
    
    if model is None:
        print("‚ùå No se pudo cargar el modelo")
        return False
    
    # Generar suite emocional completa
    results, output_dir, total_audio_duration, total_generation_time, system_stats = generate_emotional_suite_optimized(
        model, processor, tokenizer
    )
    
    # Reporte final
    save_optimized_report(results, output_dir, total_audio_duration, total_generation_time, system_stats)
    
    # Limpiar
    del model, processor, tokenizer
    torch.cuda.empty_cache()
    gc.collect()
    
    print(f"\nüéâ ¬°MISI√ìN CUMPLIDA!")
    print("=" * 80)
    print("üé≠ Elise emocional funcionando con CSM TTS")
    print("üî• Potencia completa de A100 80GB aprovechada")
    print("üéµ Audio emocional de alta calidad generado")
    print("‚ö° Optimizaci√≥n y velocidad m√°xima")
    
    return True

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
"""
Test script for CSM-1B using native Transformers API
"""
import torch
from transformers import CsmForConditionalGeneration, AutoProcessor
import numpy as np
import torchaudio
from pathlib import Path

def test_csm_native():
    """Test CSM using native Transformers API"""
    print("üé§ Testing CSM-1B with native Transformers API")
    print("=" * 60)
    
    model_id = "./models/sesame-csm-1b"
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    try:
        print(f"üîç Loading model from: {model_id}")
        print(f"üñ•Ô∏è Device: {device}")
        
        # Load the model and processor
        print("üì• Loading processor...")
        processor = AutoProcessor.from_pretrained(model_id)
        
        print("üì• Loading model...")
        model = CsmForConditionalGeneration.from_pretrained(
            model_id, 
            device_map=device,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32
        )
        
        print("‚úÖ Model and processor loaded successfully")
        
        # Test basic generation
        print("\nüéµ Testing basic generation...")
        text = "[0]Hello from CSM. This is a test of voice cloning."
        inputs = processor(text, add_special_tokens=True).to(device)
        
        print(f"üìù Text: {text}")
        print("üîÑ Generating audio...")
        
        with torch.no_grad():
            audio = model.generate(**inputs, output_audio=True, max_new_tokens=512)
        
        # Save the audio
        output_path = "output_basic_test.wav"
        processor.save_audio(audio, output_path)
        
        print(f"‚úÖ Audio generated and saved to: {output_path}")
        
        # Test with context (voice cloning)
        print("\nüé≠ Testing voice cloning with context...")
        
        # Check if we have reference audio
        reference_audio = "voices/Ah, ¬øen serio? Vaya, eso debe ser un poco inc√≥modo para tu equipo..mp3"
        if Path(reference_audio).exists():
            print(f"üìÑ Found reference audio: {reference_audio}")
            
            # Load reference audio
            reference_waveform, sr = torchaudio.load(reference_audio)
            
            # Resample to 24kHz if needed
            if sr != 24000:
                resampler = torchaudio.transforms.Resample(sr, 24000)
                reference_waveform = resampler(reference_waveform)
            
            # Convert to mono if stereo
            if reference_waveform.shape[0] > 1:
                reference_waveform = reference_waveform.mean(dim=0, keepdim=True)
            
            # Create conversation with context
            conversation = [
                {
                    "role": "0",
                    "content": [
                        {"type": "text", "text": "Ah, ¬øen serio? Vaya, eso debe ser un poco inc√≥modo para tu equipo."},
                        {"type": "audio", "path": reference_waveform.squeeze().numpy()}
                    ]
                },
                {
                    "role": "0",
                    "content": [{"type": "text", "text": "Hola, esto es una prueba de clonaci√≥n de voz con CSM."}]
                }
            ]
            
            inputs = processor.apply_chat_template(
                conversation,
                tokenize=True,
                return_dict=True,
            ).to(device)
            
            print("üîÑ Generating cloned voice...")
            with torch.no_grad():
                cloned_audio = model.generate(**inputs, output_audio=True, max_new_tokens=512)
            
            output_path_cloned = "output_cloned_test.wav"
            processor.save_audio(cloned_audio, output_path_cloned)
            
            print(f"‚úÖ Cloned audio generated and saved to: {output_path_cloned}")
        else:
            print(f"‚ö†Ô∏è Reference audio not found: {reference_audio}")
            print("üí° Skipping voice cloning test")
        
        print("\nüéâ All tests completed successfully!")
        print("=" * 60)
        print("üîß Model Information:")
        print(f"   ‚Ä¢ Model ID: {model_id}")
        print(f"   ‚Ä¢ Device: {device}")
        print(f"   ‚Ä¢ Model type: {type(model).__name__}")
        print(f"   ‚Ä¢ Processor type: {type(processor).__name__}")
        if torch.cuda.is_available():
            print(f"   ‚Ä¢ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            print(f"   ‚Ä¢ GPU Usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error during testing: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_csm_native()
    if success:
        print("\nüöÄ CSM-1B is ready for use!")
    else:
        print("\n‚ùå CSM-1B test failed")
        exit(1) 